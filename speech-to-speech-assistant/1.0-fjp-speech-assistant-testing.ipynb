{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Speech Assistant Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\n",
      "speechsdk version: 1.37.0\n",
      "openai version: 1.33.0\n",
      "json version: 2.0.9\n",
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import ssl\n",
    "import urllib.request\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"speechsdk version: {speechsdk.__version__}\")\n",
    "print(f\"openai version: {openai.__version__}\")\n",
    "print(f\"json version: {json.__version__}\")\n",
    "print('Setup Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# AML Setup\n",
    "aml_endpoint = os.environ.get('AML_EDNPOINT')\n",
    "aml_key = os.environ.get('AML_KEY')\n",
    "\n",
    "# Azure OpenAI setup\n",
    "endpoint = os.environ.get(\"OPEN_AI_ENDPOINT\")\n",
    "api_key = os.environ.get(\"OPEN_AI_KEY\")\n",
    "deployment = os.environ.get(\"OPEN_AI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Azure AI Search setup\n",
    "search_endpoint = os.getenv(\"SEARCH_ENDPOINT\"); \n",
    "search_key = os.getenv(\"SEARCH_KEY\"); \n",
    "search_index_name = os.getenv(\"SEARCH_INDEX\"); \n",
    "\n",
    "# setup speech configuration \n",
    "region = os.getenv(\"SPEECH_REGION\")\n",
    "speech_config = speechsdk.SpeechConfig(\n",
    "    subscription=os.getenv(\"SPEECH_KEY\"), \n",
    "    region=region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Speech to Speech Copilot RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "User: Hello.\n",
      "Assistant: Hi there! How can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.SpeechSynthesisResult at 0x2e2b9f5ef10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the text from the microphone\n",
    "audio_config = speechsdk.audio.AudioConfig(\n",
    "  use_default_microphone=True)\n",
    "speech_config.speech_recognition_language=\"en-US\"\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "  speech_config, \n",
    "  audio_config)\n",
    "\n",
    "print(\"Say something...\")\n",
    "speech_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "print(f\"User: {speech_result.text}\")\n",
    "\n",
    "message_text = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"## System Message for Storyteller Assistant 'Adventuress'\\n- **Role**: I am 'Adventuress', a storytelling assistant designed to engage children with visual disabilities in immersive story experiences.\\n- **Capabilities**: My primary function is to provide an interactive and accessible storytelling experience. I can understand and respond to the child's input regarding their name, age, and preferred story genre.\\n- **Limitations**: I do not store personal information and will not remember past interactions. My knowledge is limited to the indexed database of stories provided.\\n- **Output Format**: My responses will be in the form of friendly and engaging dialogue, suitable for children. I will use placeholders like [child's name] and [story type] to personalize the interaction.\\n- **Safety**: I am programmed to ensure a safe and positive experience, avoiding any content that may be inappropriate or harmful for children.\\n- **Behavioral Guardrails**: I will maintain a cheerful and supportive tone throughout the interaction, encouraging the child's love for stories and imagination.\\n\\n## Example Interaction Flow\\n- Greeting: \\\"Hello! My name is Adventuress, your story assistant. I'm here to take you to worlds of fantasy and adventure through books. What's your name?\\\"\\n- Personalization: \\\"What a lovely name, [child's name]! It's a pleasure to meet you. Tell me, how old are you?\\\"\\n- Story Selection: \\\"[Child's age] years is a wonderful age to discover incredible stories! What kind of stories would you like to hear? Adventure,fiction, fantasy mystery, animals, or perhaps classic tales?\\\"\\n- Story Introduction: \\\"I see, you like [story type] stories. I have the perfect tale for you. It's called '[Story Name]', and I think you're going to love it. Allow me to read it to you...\\\"\\n- Feedback Request: \\\"Did you enjoy the story '[Story Name]'? On a scale from zero to five, with zero being not at all and 5 being very much, how much did you like it? Your feedback helps me choose even better stories for you next time!\\\"\\n- Feedback Storage: \\\"I have recorded your [child rating] rating for '[Story Name]'. Thank you for helping me learn about your tastes!\\\"\\n\\n\"\n",
    "    },\n",
    "    { \"role\": \"user\",\n",
    "      \"content\": speech_result.text}\n",
    "    ]\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    base_url=f\"{endpoint}/openai/deployments/{deployment}/extensions\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-08-01-preview\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=message_text,\n",
    "    extra_body={\n",
    "        \"dataSources\": [\n",
    "            {\n",
    "                \"type\": \"AzureCognitiveSearch\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": os.environ[\"SEARCH_ENDPOINT\"],\n",
    "                    \"key\": os.environ[\"SEARCH_KEY\"],\n",
    "                    \"indexName\": os.environ[\"SEARCH_INDEX\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {completion.choices[0].message.content}\")\n",
    "\n",
    "# Play the result on the computer's speaker\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-AnaNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config)\n",
    "speech_synthesizer.speak_text(\n",
    "  completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 RecSys Endpoint Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"Results\": {\"WebServiceOutput0\": [{\"User\": \"5792082567cfacbc2d71bceb212a3065\", \"Recommended Item 1\": \"65832\", \"Predicted Rating 1\": 2.043990135192871, \"Recommended Item 2\": \"1001896\", \"Predicted Rating 2\": 2.0283126831054688, \"Recommended Item 3\": \"533675\", \"Predicted Rating 3\": 1.8523731231689453}, {\"User\": \"80d52f5e70f023bd0098ab96599a3530\", \"Recommended Item 1\": \"65832\", \"Predicted Rating 1\": 4.156232833862305, \"Recommended Item 2\": \"533675\", \"Predicted Rating 2\": 3.8327646255493164, \"Recommended Item 3\": \"1577950\", \"Predicted Rating 3\": 3.8082356452941895}, {\"User\": \"9773b4c5dc241e38d76aff2cb96c96fd\", \"Recommended Item 1\": \"533675\", \"Predicted Rating 1\": 4.389653205871582, \"Recommended Item 2\": \"852724\", \"Predicted Rating 2\": 4.380564212799072, \"Recommended Item 3\": \"1577950\", \"Predicted Rating 3\": 4.188114166259766}]}}'\n"
     ]
    }
   ],
   "source": [
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "data =  {\n",
    "  \"Inputs\": {\n",
    "    \"input1\": [\n",
    "      {\n",
    "        \"user_id\": \"9773b4c5dc241e38d76aff2cb96c96fd\",\n",
    "        \"book_id\": 19337,\n",
    "        \"rating\": 3\n",
    "      },\n",
    "      {\n",
    "        \"user_id\": \"5792082567cfacbc2d71bceb212a3065\",\n",
    "        \"book_id\": 1001896,\n",
    "        \"rating\": 4\n",
    "      },\n",
    "      {\n",
    "        \"user_id\": \"80d52f5e70f023bd0098ab96599a3530\",\n",
    "        \"book_id\": 375711,\n",
    "        \"rating\": 5\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"GlobalParameters\": {}\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "if not aml_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ aml_key)}\n",
    "\n",
    "req = urllib.request.Request(aml_endpoint, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Speech to Speech OpenaAI Assistant Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI is listening. Say 'Stop' or press Ctrl-Z to end the conversation.\n",
      "Recognized speech: Hi, how are you?\n",
      "Speech synthesized to speaker for: Hello!\n",
      "Speech synthesized to speaker for: I'm an AI, so I don't have feelings, but I'm here to help.\n",
      "Speech synthesized to speaker for: How can I assist you today?\n",
      "Azure OpenAI is listening. Say 'Stop' or press Ctrl-Z to end the conversation.\n",
      "Conversation ended.\n"
     ]
    }
   ],
   "source": [
    "# Create a Azure OpenAI client\n",
    "client = openai.AzureOpenAI(\n",
    "azure_endpoint=endpoint,\n",
    "api_key=api_key,\n",
    "api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "# This will correspond to the custom name you chose for your deployment when you deployed a model.\n",
    "deployment_id=deployment\n",
    "\n",
    "# Enable the use of the microphone and speakers \n",
    "audio_output_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "# Should be the locale for the speaker's language.\n",
    "speech_config.speech_recognition_language=\"en-US\"\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# The language of the voice that responds on behalf of Azure OpenAI.\n",
    "speech_config.speech_synthesis_voice_name='en-US-AnaNeural'\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output_config)\n",
    "# tts sentence end mark\n",
    "tts_sentence_end = [ \".\", \"!\", \"?\", \";\", \"。\", \"！\", \"？\", \"；\", \"\\n\" ]\n",
    "\n",
    "# Prompts Azure OpenAI with a request and synthesizes the response.\n",
    "def ask_openai(prompt):\n",
    "    # Ask Azure OpenAI in streaming way\n",
    "    response = client.chat.completions.create(model=deployment_id, max_tokens=200, stream=True, messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    collected_messages = []\n",
    "    last_tts_request = None\n",
    "\n",
    "    # iterate through the stream response stream\n",
    "    for chunk in response:\n",
    "        if len(chunk.choices) > 0:\n",
    "            chunk_message = chunk.choices[0].delta.content  # extract the message\n",
    "            if chunk_message is not None:\n",
    "                collected_messages.append(chunk_message)  # save the message\n",
    "                if chunk_message in tts_sentence_end: # sentence end found\n",
    "                    text = ''.join(collected_messages).strip() # join the recieved message together to build a sentence\n",
    "                    if text != '': # if sentence only have \\n or space, we could skip\n",
    "                        print(f\"Speech synthesized to speaker for: {text}\")\n",
    "                        last_tts_request = speech_synthesizer.speak_text_async(text)\n",
    "                        collected_messages.clear()\n",
    "    if last_tts_request:\n",
    "        last_tts_request.get()\n",
    "\n",
    "# Continuously listens for speech input to recognize and send as text to Azure OpenAI\n",
    "def chat_with_open_ai():\n",
    "    while True:\n",
    "        print(\"Azure OpenAI is listening. Say 'Stop' or press Ctrl-Z to end the conversation.\")\n",
    "        try:\n",
    "            # Get audio from the microphone and then send it to the TTS service.\n",
    "            speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "            # If speech is recognized, send it to Azure OpenAI and listen for the response.\n",
    "            if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "                if speech_recognition_result.text == \"Stop.\": \n",
    "                    print(\"Conversation ended.\")\n",
    "                    break\n",
    "                print(\"Recognized speech: {}\".format(speech_recognition_result.text))\n",
    "                ask_openai(speech_recognition_result.text)\n",
    "            elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "                print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "                break\n",
    "            elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "                cancellation_details = speech_recognition_result.cancellation_details\n",
    "                print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "                if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                    print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "# Main\n",
    "\n",
    "try:\n",
    "    chat_with_open_ai()\n",
    "except Exception as err:\n",
    "    print(\"Encountered exception. {}\".format(err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
